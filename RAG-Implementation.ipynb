{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authors\n",
    "Niklas Roslund \n",
    "\n",
    "Wiljam Wilmi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import sqlite3\n",
    "import pickle\n",
    "from scipy.spatial.distance import cosine\n",
    "import requests\n",
    "import json\n",
    "#import pandas as pd    #Used for extracting information from excel file\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings database from PDF-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def database_exists(db_path):\n",
    "    return os.path.exists(db_path)\n",
    "\n",
    "# TODO: Try other models for tokenizing?\n",
    "def initialize_model(model_name='thenlper/gte-large'):\n",
    "    \"\"\"\n",
    "    Initialize tokenizer and model for embeddings, and set the device.\n",
    "    \n",
    "    Args:\n",
    "    - model_name: The name of the model to initialize.\n",
    "    \n",
    "    Returns:\n",
    "    - tokenizer: Initialized tokenizer.\n",
    "    - model: Initialized model.\n",
    "    - device: The device being used (cuda or cpu).\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    return tokenizer, model, device\n",
    "\n",
    "def initialize_database(db_path):\n",
    "    \"\"\"Initialize SQLite database and create table.\"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS sentence_embeddings (\n",
    "                    sentence TEXT, \n",
    "                    embedding BLOB)''')\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    return extract_text(pdf_path) \n",
    "\n",
    "# This sliding window makes the chunks of text from the pdf.\n",
    "# Change chunk_size and overlap to your preferred choice.\n",
    "def sliding_window_tokenize(text, chunk_size=300, overlap=200):\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(text) - chunk_size:\n",
    "        chunks.append(text[i:i+chunk_size])\n",
    "        i += (chunk_size - overlap)\n",
    "    chunks.append(text[i:])  # Add the last chunk\n",
    "    return chunks\n",
    "\n",
    "# The relevant terms are supposed to be more than \"sustainability\" but other keywords have been omitted.\n",
    "def clean_sentences(sentences, min_length=5, relevant_terms = {'sustainability'}):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Remove non-textual elements and correct formatting\n",
    "        sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
    "\n",
    "        # Filter out too short or non-informative sentences\n",
    "        if len(sentence.split()) < min_length:\n",
    "            continue\n",
    "        \n",
    "        # Semantic filtering based on key terms\n",
    "        if not any(term in sentence.lower() for term in relevant_terms):\n",
    "            continue\n",
    "\n",
    "        # Add the cleaned, relevant sentence\n",
    "        cleaned_sentences.append(sentence)\n",
    "    \n",
    "    # Deduplication\n",
    "    unique_sentences = list(set(cleaned_sentences))\n",
    "    \n",
    "    return unique_sentences\n",
    "\n",
    "def generate_and_store_embeddings(text, db_path, tokenizer, model, device):\n",
    "    sentences = sliding_window_tokenize(text)\n",
    "    sentences = clean_sentences(sentences)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    c = conn.cursor()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        sentence_embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "        emb_blob = pickle.dumps(sentence_embedding.cpu().numpy())\n",
    "        \n",
    "        c.execute(\"INSERT INTO sentence_embeddings (sentence, embedding) VALUES (?, ?)\", (sentence, emb_blob))\n",
    "    \n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose prefix for which document is to be read.\n",
    "# Here you can choose which company you want to evaluate. \n",
    "# Preset to do all 50 included in the project\n",
    "\n",
    "company_names = []\n",
    "company_names = (\"Afry\", \"Ageas\", \"Airbus\", \"AkerSolutions\", \"Amcor\", \"Arla\", \"Barclays\", \"Betsson\", \"BlackRock\", \"Carlsberg\", \"Colruyt\", \"Diageo\", \"Dufry\", \"Eaton\", \"Elanders\", \"Fortum\", \"Glencore\", \"Granges\", \"Hays\", \"Husqvarna\", \"Hydro\", \"Inditex\", \"JeronimoMartins\", \"Kerry\", \"Leonardo\", \"Lundbeck\", \"Maersk\", \"NationalGrid\", \"NatwestHoldings\", \"Norwegian\", \"Orange\", \"Peab\", \"Postnord\", \"Randstad\", \"Richemont\", \"Rockwool\", \"RollsRoyce\", \"RoyalMail\", \"Securitas\", \"Siemens\", \"Storebrand\", \"Telefonica\", \"Tesco\", \"Thyssenkrupp\", \"Vattenfall\", \"Viterra\", \"Vodafone\", \"Rewe\", \"Preem\", \"BASF\")\n",
    "\n",
    "for company_name in company_names:\n",
    "\n",
    "    # If you have other files these paths might need to change\n",
    "    data_path = f\"data/{company_name}_2022.pdf\"\n",
    "    db_path = f'embeddings/{company_name}_embeddings.db'\n",
    "\n",
    "    if not database_exists(db_path):\n",
    "        # Generate embeddings\n",
    "        tokenizer, model, device = initialize_model()\n",
    "        initialize_database(db_path)\n",
    "        extracted_text = extract_text_from_pdf(data_path)\n",
    "        generate_and_store_embeddings(extracted_text, db_path, tokenizer, model, device)\n",
    "    else:\n",
    "        print(f\"Embeddings database for {company_name} already exists. Skipping embeddings generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you add all the questions for the LLM.\n",
    "# The real questions have been omitted.\n",
    "\n",
    "questions = []\n",
    "questions.append(\"does the report mention sustainability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you add all relevant keywords for the question.\n",
    "# Be sure to have same amount of keyword items as question items as they are paired.\n",
    "# Same here, all the real keywords have been omitted.\n",
    "\n",
    "keywords = []\n",
    "keywords.append(\"sustainability, report, policy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve and Analyze embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    \"\"\"Calculate the cosine similarity between two vectors.\"\"\"\n",
    "    return 1 - cosine(v1, v2)\n",
    "\n",
    "def connect_to_db(company_name):\n",
    "    \"\"\"Establish a connection to the SQLite database.\"\"\"\n",
    "    conn = sqlite3.connect(f'embeddings/{company_name}_embeddings.db')\n",
    "    return conn\n",
    "\n",
    "def fetch_embeddings(conn):\n",
    "    \"\"\"Fetch all sentences and embeddings from the database.\"\"\"\n",
    "    c = conn.cursor()\n",
    "    c.execute(\"SELECT sentence, embedding FROM sentence_embeddings\")\n",
    "    return c.fetchall()\n",
    "\n",
    "def convert_query_to_embedding(query, tokenizer, model, device):\n",
    "    \"\"\"Convert query into embedding.\"\"\"\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "def calculate_similarities_old(rows, query_embedding):\n",
    "    \"\"\"Calculate cosine similarities between query embedding and database embeddings.\"\"\"\n",
    "    similarities = []\n",
    "    for sentence, emb_blob in rows:\n",
    "        embedding = pickle.loads(emb_blob).flatten()\n",
    "        similarity = cosine_similarity(query_embedding.flatten(), embedding)\n",
    "        similarities.append((sentence, similarity))\n",
    "    return similarities\n",
    "\n",
    "def calculate_similarities(rows, query_embeddings):\n",
    "    \"\"\"Calculate average cosine similarities between multiple query embeddings and database embeddings.\"\"\"\n",
    "    similarities = []\n",
    "    for sentence, emb_blob in rows:\n",
    "        embedding = pickle.loads(emb_blob).flatten()\n",
    "        individual_similarities = [cosine_similarity(query_emb.flatten(), embedding) for query_emb in query_embeddings]\n",
    "        average_similarity = sum(individual_similarities) / len(individual_similarities)\n",
    "        similarities.append((sentence, average_similarity))\n",
    "    return similarities\n",
    "\n",
    "\n",
    "def select_top_matches(similarities, top_n):\n",
    "    \"\"\"Select top N results based on similarity.\"\"\"\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "def generate_corpus(top_matches):\n",
    "    \"\"\"Generate a corpus of documents based on the top matches.\"\"\"\n",
    "    corpus = [f\"Sentence: {sentence}\" for sentence, _ in top_matches]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many documents the model should look at\n",
    "nr_of_documents = 5\n",
    "documents_by_company = []\n",
    "tokenizer, model, device = initialize_model()\n",
    "\n",
    "for company_name in company_names:\n",
    "    corpus_of_documents = []\n",
    "    conn = connect_to_db(company_name)\n",
    "    rows = fetch_embeddings(conn)\n",
    "    for question, keyword in zip(questions, keywords):\n",
    "        query_embedding = convert_query_to_embedding(keyword, tokenizer, model, device) \n",
    "        similarities = calculate_similarities(rows, query_embedding)\n",
    "        top_matches = select_top_matches(similarities, nr_of_documents)\n",
    "        corpus_of_documents.append(generate_corpus(top_matches))\n",
    "    documents_by_company.append(corpus_of_documents)\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "# Displaying the top matches\n",
    "#for doc in corpus_of_documents:\n",
    "#    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenates the five documents into one element.\n",
    "company_relevant_documents_text = []\n",
    "for company_documents in documents_by_company:\n",
    "    relevant_documents_text = []\n",
    "    for corpus in company_documents:\n",
    "        relevant_document = corpus[:nr_of_documents]\n",
    "        relevant_documents_text.append(\"\\n\\n---\\n\\n\".join(relevant_document))\n",
    "    company_relevant_documents_text.append(relevant_documents_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you create the prompt and it is possible to change this to alter the results.\n",
    "# Some of the information has been removed from the prompt but this is the shell to make it work.\n",
    "\n",
    "def getPrompts(relevant_documents_text, questions):\n",
    "    prompts = []\n",
    "    prompts.append(\n",
    "        f\"\"\"\n",
    "    Context:\n",
    "    Your task is to determine if the provided documents adequately address the posed query. \n",
    "                   \n",
    "    Directly answer 'No' or 'Yes' at the beginning of your response. No introductory phrases are needed.\n",
    "    If uncertain, answer 'No'.\n",
    "    \n",
    "    Documents provided:\n",
    "    {relevant_documents_text[0]}\n",
    "\n",
    "    Question:\n",
    "    {questions[0]}\n",
    "\n",
    "    \"\"\")\n",
    "    return prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating answers from the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ollama API call (ollama needs to be running on computer with selected model downloaded)\n",
    "# In this instance you need to download Ollama and mistral:instruct\n",
    "# It is possible to change the model just by changing the name \"model\": \"mistral:instruct\"\n",
    "# https://ollama.com/library/mistral:instruct\n",
    "\n",
    "url = 'http://localhost:11434/api/generate'\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "company_responses = []  # This will be a list of dictionaries\n",
    "for company_name, document_by_company in zip(company_names, documents_by_company):\n",
    "    prompts = getPrompts(document_by_company, questions)\n",
    "    transformed_responses = []  # To store Yes/No as 1/0 for current company\n",
    "    combined_response = []\n",
    "    output_ = []\n",
    "    for prompt, question, relevant_document in zip(prompts, questions, document_by_company):\n",
    "        data = {\n",
    "            # Here you change the model\n",
    "            \"model\": \"mistral:instruct\",\n",
    "            \"prompt\": prompt.format(question=question, relevant_document=relevant_document)\n",
    "        }\n",
    "        response = requests.post(url, data=json.dumps(data), headers=headers, stream=True)\n",
    "        full_response = []\n",
    "        try:\n",
    "            for line in response.iter_lines():\n",
    "                if line:\n",
    "                    decoded_line = json.loads(line.decode('utf-8'))\n",
    "                    if 'response' in decoded_line:\n",
    "                        full_response.append(decoded_line['response'])\n",
    "        finally:\n",
    "            response.close()\n",
    "\n",
    "        # Join the collected lines into a single string and extract the first line\n",
    "        combined_response = ''.join(full_response)\n",
    "        first_line = full_response[0].strip().lower()\n",
    "        #first_line = combined_response('\\n', 1)[0].strip().lower()  # Strip to remove whitespace\n",
    "        normalized_line = first_line.replace(\"*\", \"\").replace(\"#\", \"\")\n",
    "        output_.append(combined_response)\n",
    "        \n",
    "        # This code is still a bit bad because sometimes the model doesnt answer with yes or no in the beginning\n",
    "        # Some models like to answer \"based on....\" in that case we just give the answer a \"no\" or 0.\n",
    "        # This rarely happens for some models and some do it all the time so important to check these.\n",
    "        # Might be possible with other methids but this one was best for our purpose.\n",
    "        if \"no\" in normalized_line:\n",
    "            print(\"no = \" + normalized_line)\n",
    "            answer = 0\n",
    "        elif \"yes\" in normalized_line:\n",
    "            print(\"yes = \" + normalized_line)\n",
    "            answer = 1\n",
    "        else:\n",
    "            print(\"no = \" + normalized_line)\n",
    "            answer = 0\n",
    "        \n",
    "        transformed_responses.append(answer)\n",
    "    print(company_name)\n",
    "    for a in output_:\n",
    "        print(a)\n",
    "    # After collecting all responses for the current company, append to company_responses\n",
    "    company_responses.append({'company_name': company_name, 'questions_answers': transformed_responses})\n",
    "\n",
    "\n",
    "# You will get all the answers under here first one time without the rationale and just the yes or no answers.\n",
    "# Second time you will recieve full answers with the rationale from the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction of data from Excel\n",
    "#### After this we have a method to evaluate the models against old data but the data has been omitted so the code for evaluation is still there but the data is not so the code won't run. However, if you run new documents they will not have the real answers so this evaluation is only for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code will not run without the data but will be kept if anyone is intrested.\n",
    "# It would print out confusion matrix, classification report and accuracy of the model.\n",
    "\n",
    "\n",
    "company_responses_dict = {item['company_name']: item['questions_answers'] for item in company_responses}\n",
    "\n",
    "results = []\n",
    "all_true = []\n",
    "all_pred = []\n",
    "\n",
    "# Iterate through structured_data to match and calculate metrics for company_responses\n",
    "for structured_item in structured_data:\n",
    "    company_name = structured_item['company_name']\n",
    "    if company_name in company_responses_dict:\n",
    "        true_answers = structured_item['questions_answers']\n",
    "        pred_answers_raw = company_responses_dict[company_name]\n",
    "\n",
    "        # Handle None values by converting them to 0\n",
    "        pred_answers = [0 if answer is None else answer for answer in pred_answers_raw]\n",
    "\n",
    "        # Append answers for overall accuracy calculation\n",
    "        all_true.extend(true_answers)\n",
    "        all_pred.extend(pred_answers)\n",
    "\n",
    "        # Calculate accuracy for each company\n",
    "        if len(true_answers) == len(pred_answers):\n",
    "            company_accuracy = accuracy_score(true_answers, pred_answers)\n",
    "            results.append({\n",
    "                'company_name': company_name,\n",
    "                'accuracy': company_accuracy\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Length mismatch for {company_name}\")\n",
    "\n",
    "# Overall accuracy and other metrics\n",
    "overall_accuracy = accuracy_score(all_true, all_pred)\n",
    "print(f\"Overall Accuracy: {overall_accuracy}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(all_true, all_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(all_true, all_pred))\n",
    "\n",
    "# Output the results for each company\n",
    "for result in results:\n",
    "    print(result)\n",
    "\n",
    "# Now let's handle the question-wise accuracy\n",
    "num_questions = len(structured_data[0]['questions_answers'])  # Assuming all entries have the same number of questions\n",
    "all_true_by_question = [[] for _ in range(num_questions)]\n",
    "all_pred_by_question = [[] for _ in range(num_questions)]\n",
    "\n",
    "# Populate the lists for each question\n",
    "for structured_item in structured_data:\n",
    "    company_name = structured_item['company_name']\n",
    "    if company_name in company_responses_dict:\n",
    "        true_answers = structured_item['questions_answers']\n",
    "        pred_answers = [0 if answer is None else answer for answer in company_responses_dict[company_name]]\n",
    "\n",
    "        if len(true_answers) == len(pred_answers):\n",
    "            for i in range(num_questions):\n",
    "                all_true_by_question[i].append(true_answers[i])\n",
    "                all_pred_by_question[i].append(pred_answers[i])\n",
    "print(\"\\n\")\n",
    "# Calculate and print accuracy for each question\n",
    "for i in range(num_questions):\n",
    "    question_accuracy = accuracy_score(all_true_by_question[i], all_pred_by_question[i])\n",
    "    print(f\"Accuracy for Question {i + 1}: {question_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
